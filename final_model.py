# -*- coding: utf-8 -*-
"""1920(FTR).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RP4dJqheId0evBV3rvyeHHOtrilSQCUm
"""

from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import  LogisticRegression
from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.neighbors import KNeighborsClassifier as KNN
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import xgboost as xgb
from sklearn.metrics import accuracy_score,confusion_matrix

df=pd.read_csv('PL2019_with_dummies .csv')

df.head()

df.shape

"""deleting them """

X,y=df.iloc[:,:-1], df.iloc[:,-1]

df.describe()

#while taking desision

X.head()

X_train.head()

X_train.drop(['Outlier'],axis=1,inplace=True)
X_test.drop(['Outlier'],axis=1,inplace=True)

"""XG boost

trai test split
"""

X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2,stratify=y, random_state=123)

xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100, seed=123)

xg_cl.fit(X_train,y_train)

# Predict the labels of the test set: preds
predict = xg_cl.predict(X_test)

acc = accuracy_score(predict, y_test)
print("Test set accuracy: {:.2f}".format(acc))



"""Bagging CLASSIFIER"""

from sklearn.tree import DecisionTreeClassifier

from sklearn.ensemble import BaggingClassifier

# Instantiate dt
dt = DecisionTreeClassifier(min_samples_leaf=4,max_depth=5, random_state=1)

# Instantiate bc
bc = BaggingClassifier(base_estimator=dt, 
            n_estimators=50,
            oob_score=True,
            random_state=1)

# Fit bc to the training set 
bc.fit(X_train,y_train)

# Predict test set labels
y_pred = bc.predict(X_test)

# Evaluate test set accuracy
acc_test = accuracy_score(y_pred, y_test)

# Evaluate OOB accuracy
acc_oob = bc.oob_score_

# Print acc_test and acc_oob
print('Test set accuracy: {:.3f}, OOB accuracy: {:.3f}'.format(acc_test, acc_oob))

print(xg_cl.feature_importances_)

import matplotlib.pyplot as plt

importances = pd.Series(data=xg_cl.feature_importances_,
                        index= X_train.columns)

# Sort importances
importances_sorted = importances.sort_values()

# Draw a horizontal barplot of importances_sorted
importances_sorted.plot(kind='barh', color='lightgreen')
plt.title('Features Importances')
plt.show()

from matplotlib.pyplot import figure
figure(num=None, figsize=(40, 40), dpi=80, facecolor='w', edgecolor='k')
importances_sorted.plot(kind='barh', color='lightgreen')
plt.title('Features Importances')
plt.show()

from sklearn.ensemble import RandomForestClassifier

